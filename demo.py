from flask import Flask, render_template, Response, jsonify
import cv2
from threading import Thread, Lock
import time
import face_recognition
import numpy as np
import datetime
import pygame
import requests  # Th√™m th∆∞ vi·ªán requests ƒë·ªÉ g·ª≠i HTTP request

app = Flask(__name__)

CAM_A_IP = 'http://192.168.1.101:4747/video'
CAM_B_IP = 'http://192.168.1.102:4747/video'
CAM_C_IP = 'http://192.168.1.110:4747/video'  # Th√™m camera C

width, height = 160, 120  # Gi·∫£m k√≠ch th∆∞·ªõc khung h√¨nh ƒë·ªÉ tƒÉng t·ªëc

lock = Lock()
frame_a, frame_b, frame_c = None, None, None

time_a_start = None
time_b_detected = None
time_c_detected = None
face_detected_a = False
cycle_completed = False

max_travel_time = 5
min_travel_time = 2
min_valid_time = 0.5

# Load face encodings
from face_recognition_train import load_encodings
try:
    known_encodings, known_labels = load_encodings("face_encodings.npz")
    print(f"Loaded {len(known_encodings)} face encodings")
except Exception as e:
    print(f"Error loading face encodings: {e}")
    known_encodings = []
    known_labels = []

last_processed_time = datetime.datetime.now()
MOCKAPI_URL = "https://6788b3fc2c874e66b7d5f86e.mockapi.io/runner"

# Kh·ªüi t·∫°o pygame mixer
pygame.mixer.init()
alert_sound = "static/sounds/alert.mp3"
ting_ting_sound = "static/sounds/ting.mp3"

def play_alert():
    pygame.mixer.music.load(alert_sound)
    pygame.mixer.music.play()

def play_ting_ting():
    pygame.mixer.music.load(ting_ting_sound)
    pygame.mixer.music.play()

# Global variable to track completion state and timestamp
completion_state = {
    "message": "",
    "timestamp": None
}

class CameraStream:
    def __init__(self, src):
        self.cap = cv2.VideoCapture(src)
        self.lock = Lock()
        self.frame = None
        self.running = True
        self.motion_detector = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=False)
        self.previous_frame = None
        self.motion_detected = False
        self.thread = Thread(target=self.update, daemon=True)
        self.thread.start()

    def update(self):
        while self.running:
            ret, frame = self.cap.read()
            if ret:
                with self.lock:
                    self.frame = cv2.resize(frame, (width, height))
                    
    def detect_motion(self, frame):
        # Apply background subtraction
        fg_mask = self.motion_detector.apply(frame)
        
        # Threshold to get binary image
        _, thresh = cv2.threshold(fg_mask, 127, 255, cv2.THRESH_BINARY)
        
        # Calculate the percentage of moving pixels
        motion_pixels = np.count_nonzero(thresh)
        total_pixels = thresh.size
        motion_percentage = (motion_pixels / total_pixels) * 100
        
        # Return True if motion percentage is above threshold
        return motion_percentage > 1.5  # Adjust threshold as needed

    def get_frame(self):
        with self.lock:
            if self.frame is not None:
                frame_copy = self.frame.copy()
                # Update motion detection
                self.motion_detected = self.detect_motion(frame_copy)
                return frame_copy
            return None

    def is_motion_detected(self):
        return self.motion_detected

    def stop(self):
        self.running = False
        self.cap.release()

camera_a = CameraStream(CAM_A_IP)
camera_b = CameraStream(CAM_B_IP)
camera_c = CameraStream(CAM_C_IP)  # Th√™m camera C

def detect_and_recognize_faces(frame, interval=0.3):
    """Nh·∫≠n di·ªán khu√¥n m·∫∑t s·ª≠ d·ª•ng encodings ƒë√£ trained"""
    global last_processed_time, face_detected_a, cycle_completed, known_encodings, known_labels
    
    now = datetime.datetime.now()
    if (now - last_processed_time).total_seconds() < interval:
        return face_detected_a, [], None  # Th√™m None cho detected_name
    last_processed_time = now

    if len(known_encodings) == 0:
        return False, [], None

    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    face_locations = face_recognition.face_locations(rgb_frame, model="hog")
    
    if len(face_locations) > 0:
        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
        
        for face_encoding in face_encodings:
            # So s√°nh v·ªõi t·∫•t c·∫£ encodings ƒë√£ bi·∫øt
            matches = face_recognition.compare_faces(known_encodings, face_encoding, tolerance=0.5)
            
            if True in matches:
                # L·∫•y index c·ªßa encoding ph√π h·ª£p nh·∫•t
                face_distances = face_recognition.face_distance(known_encodings, face_encoding)
                best_match_index = np.argmin(face_distances)
                
                if matches[best_match_index]:
                    detected_name = known_labels[best_match_index]
                    if not face_detected_a:
                        face_detected_a = True
                        print(f"Detected person: {detected_name}")
                    return True, face_locations, detected_name

    if face_detected_a:
        face_detected_a = False
        global time_a_start
        time_a_start = None
        cycle_completed = False
        
    return False, [], None

def save_to_mockapi(start_time, end_time, travel_time_a_b, travel_time_b_c, travel_time_c_a):
    """
    G·ª≠i d·ªØ li·ªáu l√™n MockAPI th√¥ng qua HTTP POST request.
    """
    data = {
        "start_time": start_time,
        "end_time": end_time,
        "travel_time_a_b": travel_time_a_b,
        "travel_time_b_c": travel_time_b_c,
        "travel_time_c_a": travel_time_c_a
    }
    try:
        response = requests.post(MOCKAPI_URL, json=data)
        if response.status_code == 201:  # 201 l√† m√£ tr·∫°ng th√°i th√†nh c√¥ng khi t·∫°o m·ªõi
            print("D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u l√™n MockAPI th√†nh c√¥ng!")
        else:
            print(f"L·ªói khi l∆∞u d·ªØ li·ªáu: {response.status_code} - {response.text}")
    except Exception as e:
        print(f"L·ªói k·∫øt n·ªëi ƒë·∫øn MockAPI: {e}")

def process_frame(camera, camera_name):
    global frame_a, frame_b, frame_c, time_a_start, time_b_detected, time_c_detected, timer_enabled, waiting_for_start
    while True:
        frame = camera.get_frame()
        if frame is None:
            time.sleep(0.05)
            continue

        current_time = time.time()
        if camera_name == "A":
            # Camera A uses face recognition
            recognized, face_locations = detect_and_recognize_faces(frame)
            if recognized:
                with lock:
                    if time_c_detected is not None:
                        # T√≠nh to√°n th·ªùi gian di chuy·ªÉn t·ª´ C v·ªÅ A
                        travel_time_c_a = current_time - time_c_detected
                        if travel_time_c_a >= min_valid_time:
                            # T√≠nh to√°n c√°c th·ªùi gian kh√°c
                            travel_time_a_b = time_b_detected - time_a_start
                            travel_time_b_c = time_c_detected - time_b_detected
                            
                            # L∆∞u d·ªØ li·ªáu
                            save_to_mockapi(
                                datetime.datetime.fromtimestamp(time_a_start).isoformat(),
                                datetime.datetime.fromtimestamp(current_time).isoformat(),
                                travel_time_a_b,
                                travel_time_b_c,
                                travel_time_c_a
                            )
                            
                            # In th√¥ng tin
                            print(f"Th·ªùi gian di chuy·ªÉn A ‚Üí B: {travel_time_a_b:.2f}s")
                            print(f"Th·ªùi gian di chuy·ªÉn B ‚Üí C: {travel_time_b_c:.2f}s")
                            print(f"Th·ªùi gian di chuy·ªÉn C ‚Üí A: {travel_time_c_a:.2f}s")
                            
                            # C·∫≠p nh·∫≠t tr·∫°ng th√°i ho√†n th√†nh
                            completion_state["message"] = "üéâ Ho√†n th√†nh chu tr√¨nh!"
                            completion_state["timestamp"] = current_time
                            
                            # ƒê√°nh d·∫•u ho√†n th√†nh chu tr√¨nh v√† reset c√°c bi·∫øn
                            cycle_completed = True
                            time_a_start = None
                            time_b_detected = None
                            time_c_detected = None
                    elif time_a_start is None:
                        time_a_start = current_time
                        print(f"B·∫Øt ƒë·∫ßu t·ª´ A: {datetime.datetime.fromtimestamp(time_a_start).isoformat()}")
                        print(f"B·∫Øt ƒë·∫ßu t·ª´ A: {datetime.datetime.fromtimestamp(time_a_start).isoformat()}")
                        
                # V·∫Ω bounding box v√† t√™n ng∆∞·ªùi ƒë∆∞·ª£c nh·∫≠n di·ªán
                for (top, right, bottom, left) in face_locations:
                    cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
                    label = detected_name if detected_name else "Unknown"
                    cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

        else:
            # Cameras B and C use motion detection
            motion_detected = camera.is_motion_detected()
            if motion_detected:
                if camera_name == "B":
                    with lock:
                        if time_a_start is not None and time_b_detected is None:
                            if current_time - time_a_start >= min_valid_time:
                                time_b_detected = current_time
                                travel_time = time_b_detected - time_a_start
                                print(f"Th·ªùi gian di chuy·ªÉn A ‚Üí B: {travel_time:.2f}s")
                                if travel_time <= min_travel_time:
                                    play_ting_ting()
                                elif travel_time > max_travel_time:
                                    play_alert()
                                # V·∫Ω th√¥ng b√°o chuy·ªÉn ƒë·ªông
                                cv2.putText(frame, "Motion Detected", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                
                elif camera_name == "C":
                    with lock:
                        if time_b_detected is not None and time_c_detected is None:
                            if current_time - time_b_detected >= min_valid_time:
                                time_c_detected = current_time
                                travel_time = time_c_detected - time_b_detected
                                print(f"Th·ªùi gian di chuy·ªÉn B ‚Üí C: {travel_time:.2f}s")
                                if travel_time <= min_travel_time:
                                    play_ting_ting()
                                elif travel_time > max_travel_time:
                                    play_alert()
                                # V·∫Ω th√¥ng b√°o chuy·ªÉn ƒë·ªông
                                cv2.putText(frame, "Motion Detected", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

        with lock:
            if camera_name == "A":
                frame_a = frame.copy()
            elif camera_name == "B":
                frame_b = frame.copy()
            elif camera_name == "C":
                frame_c = frame.copy()

def gen_frames(camera_name):
    global frame_a, frame_b, frame_c
    
    while True:
        with lock:
            frame = frame_a if camera_name == "A" else frame_b if camera_name == "B" else frame_c
            if frame is None:
                time.sleep(0.1)
                continue

            # Chuy·ªÉn ƒë·ªïi frame sang ƒë·ªãnh d·∫°ng BGR ƒë·ªÉ hi·ªÉn th·ªã
            display_frame = frame.copy()

            if camera_name == "A":
                # Camera A uses face detection
                recognized, face_locations, detected_name = detect_and_recognize_faces(display_frame)
                if recognized:
                    for (top, right, bottom, left) in face_locations:
                        cv2.rectangle(display_frame, (left, top), (right, bottom), (0, 255, 0), 2)
                        name = detected_name if detected_name else "Unknown"
                        cv2.putText(display_frame, f"{name} Detected", (left, top - 10),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            else:
                # Display motion detection status for cameras B and C
                camera = camera_b if camera_name == "B" else camera_c
                if camera.is_motion_detected():
                    cv2.putText(display_frame, "Motion Detected", (10, 30),
                              cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            # Add detection mode label
            label = "Face Detection" if camera_name == "A" else "Motion Detection"
            cv2.putText(display_frame, label, (10, display_frame.shape[0] - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

            # Encode frame th√†nh JPEG
            ret, buffer = cv2.imencode('.jpg', display_frame)
            if not ret:
                continue

        # Tr·∫£ v·ªÅ d·ªØ li·ªáu h√¨nh ·∫£nh d∆∞·ªõi d·∫°ng stream
        yield (b'--frame\r\n' 
               b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')

@app.route('/current_time')
def current_time():
    global time_a_start, face_detected_a, cycle_completed
    # Ch·ªâ b·∫Øt ƒë·∫ßu timer khi ph√°t hi·ªán khu√¥n m·∫∑t v√† ch∆∞a ho√†n th√†nh chu tr√¨nh
    if face_detected_a and time_a_start is None and not cycle_completed:
        time_a_start = time.time()
    
    if time_a_start is not None:
        elapsed_time = time.time() - time_a_start
        return jsonify({"elapsed_time": elapsed_time})
    return jsonify({"elapsed_time": None})

@app.route('/face_status')
def face_status():
    global face_detected_a, time_a_start
    current_time = time.time()
    
    # L·∫•y t√™n ng∆∞·ªùi ƒë∆∞·ª£c nh·∫≠n di·ªán t·ª´ frame m·ªõi nh·∫•t
    _, _, detected_name = detect_and_recognize_faces(frame_a if frame_a is not None else np.zeros((height, width, 3), dtype=np.uint8))
    
    status = {
        "detected": face_detected_a,
        "message": f"‚úÖ ƒê√£ x√°c nh·∫≠n {detected_name if detected_name else 'Unknown'} - ƒêang ƒë·∫øm th·ªùi gian" if face_detected_a else "‚è≥ Ch·ªù x√°c nh·∫≠n",
        "time_started": time_a_start is not None,
        "elapsed": current_time - time_a_start if time_a_start is not None else None,
        "detected_person": detected_name if detected_name else None
    }
    return jsonify(status)

@app.route('/video_feed_a')
def video_feed_a():
    return Response(gen_frames("A"), mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/video_feed_b')
def video_feed_b():
    return Response(gen_frames("B"), mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/video_feed_c')
def video_feed_c():
    return Response(gen_frames("C"), mimetype='multipart/x-mixed-replace; boundary=frame')

def calculate_travel_time():
    global time_a_start, time_b_detected, time_c_detected
    if time_a_start and time_b_detected and time_c_detected:
        travel_time_a_b = time_b_detected - time_a_start
        travel_time_b_c = time_c_detected - time_b_detected
        travel_time_c_a = time.time() - time_c_detected
        return f"‚úÖ A ‚Üí B: {travel_time_a_b:.2f}s, B ‚Üí C: {travel_time_b_c:.2f}s, C ‚Üí A: {travel_time_c_a:.2f}s"
    return "‚è≥ ƒêang thu th·∫≠p d·ªØ li·ªáu..."

@app.route('/completion_status')
def get_completion_status():
    global completion_state
    if completion_state["timestamp"] is not None:
        if time.time() - completion_state["timestamp"] > 5:
            completion_state = {"message": "", "timestamp": None}
    return jsonify(completion_state)

@app.route('/travel_time')
def travel_time():
    return jsonify({"message": calculate_travel_time()})

@app.route('/')
def index():
    return render_template('index.html')

if __name__ == '__main__':
    thread_a = Thread(target=process_frame, args=(camera_a, "A"), daemon=True)
    thread_b = Thread(target=process_frame, args=(camera_b, "B"), daemon=True)
    thread_c = Thread(target=process_frame, args=(camera_c, "C"), daemon=True)
    thread_a.start()
    thread_b.start()
    thread_c.start()
    try:
        app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)
    finally:
        camera_a.stop()
        camera_b.stop()
        camera_c.stop()